{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aQk0u3YESqbQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting diffusers[torch]\n",
            "  Using cached diffusers-0.31.0-py3-none-any.whl (2.9 MB)\n",
            "Collecting Pillow\n",
            "  Using cached pillow-11.0.0-cp310-cp310-win_amd64.whl (2.6 MB)\n",
            "Collecting requests\n",
            "  Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "Collecting filelock\n",
            "  Using cached filelock-3.16.1-py3-none-any.whl (16 kB)\n",
            "Collecting importlib-metadata\n",
            "  Using cached importlib_metadata-8.5.0-py3-none-any.whl (26 kB)\n",
            "Collecting numpy\n",
            "  Using cached numpy-2.2.1-cp310-cp310-win_amd64.whl (12.9 MB)\n",
            "Collecting huggingface-hub>=0.23.2\n",
            "  Using cached huggingface_hub-0.27.0-py3-none-any.whl (450 kB)\n",
            "Collecting safetensors>=0.3.1\n",
            "  Using cached safetensors-0.4.5-cp310-none-win_amd64.whl (285 kB)\n",
            "Collecting regex!=2019.12.17\n",
            "  Using cached regex-2024.11.6-cp310-cp310-win_amd64.whl (274 kB)\n",
            "Collecting accelerate>=0.31.0\n",
            "  Using cached accelerate-1.2.1-py3-none-any.whl (336 kB)\n",
            "Collecting torch<2.5.0,>=1.4\n",
            "  Using cached torch-2.4.1-cp310-cp310-win_amd64.whl (199.4 MB)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\sarfa\\documents\\pythonproject\\create-synthetic-dataset-emotion-recognition\\myenv\\lib\\site-packages (from accelerate>=0.31.0->diffusers[torch]) (24.2)\n",
            "Collecting pyyaml\n",
            "  Using cached PyYAML-6.0.2-cp310-cp310-win_amd64.whl (161 kB)\n",
            "Requirement already satisfied: psutil in c:\\users\\sarfa\\documents\\pythonproject\\create-synthetic-dataset-emotion-recognition\\myenv\\lib\\site-packages (from accelerate>=0.31.0->diffusers[torch]) (6.1.1)\n",
            "Collecting fsspec>=2023.5.0\n",
            "  Using cached fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "Collecting tqdm>=4.42.1\n",
            "  Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\sarfa\\documents\\pythonproject\\create-synthetic-dataset-emotion-recognition\\myenv\\lib\\site-packages (from huggingface-hub>=0.23.2->diffusers[torch]) (4.12.2)\n",
            "Collecting networkx\n",
            "  Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
            "Collecting sympy\n",
            "  Using cached sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
            "Collecting jinja2\n",
            "  Using cached jinja2-3.1.5-py3-none-any.whl (134 kB)\n",
            "Requirement already satisfied: colorama in c:\\users\\sarfa\\documents\\pythonproject\\create-synthetic-dataset-emotion-recognition\\myenv\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.23.2->diffusers[torch]) (0.4.6)\n",
            "Collecting zipp>=3.20\n",
            "  Using cached zipp-3.21.0-py3-none-any.whl (9.6 kB)\n",
            "Collecting MarkupSafe>=2.0\n",
            "  Using cached MarkupSafe-3.0.2-cp310-cp310-win_amd64.whl (15 kB)\n",
            "Collecting idna<4,>=2.5\n",
            "  Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
            "Collecting charset-normalizer<4,>=2\n",
            "  Using cached charset_normalizer-3.4.0-cp310-cp310-win_amd64.whl (102 kB)\n",
            "Collecting urllib3<3,>=1.21.1\n",
            "  Using cached urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
            "Collecting certifi>=2017.4.17\n",
            "  Using cached certifi-2024.12.14-py3-none-any.whl (164 kB)\n",
            "Collecting mpmath<1.4,>=1.1.0\n",
            "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "Installing collected packages: urllib3, mpmath, MarkupSafe, idna, charset-normalizer, certifi, zipp, tqdm, sympy, requests, pyyaml, networkx, jinja2, fsspec, filelock, torch, safetensors, regex, Pillow, numpy, importlib-metadata, huggingface-hub, diffusers, accelerate\n",
            "Successfully installed MarkupSafe-3.0.2 Pillow-11.0.0 accelerate-1.2.1 certifi-2024.12.14 charset-normalizer-3.4.0 diffusers-0.31.0 filelock-3.16.1 fsspec-2024.12.0 huggingface-hub-0.27.0 idna-3.10 importlib-metadata-8.5.0 jinja2-3.1.5 mpmath-1.3.0 networkx-3.4.2 numpy-2.2.1 pyyaml-6.0.2 regex-2024.11.6 requests-2.32.3 safetensors-0.4.5 sympy-1.13.3 torch-2.4.1 tqdm-4.67.1 urllib3-2.2.3 zipp-3.21.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Collecting transformers\n",
            "  Using cached transformers-4.47.1-py3-none-any.whl (10.1 MB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\sarfa\\documents\\pythonproject\\create-synthetic-dataset-emotion-recognition\\myenv\\lib\\site-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\sarfa\\documents\\pythonproject\\create-synthetic-dataset-emotion-recognition\\myenv\\lib\\site-packages (from transformers) (2.2.1)\n",
            "Collecting tokenizers<0.22,>=0.21\n",
            "  Using cached tokenizers-0.21.0-cp39-abi3-win_amd64.whl (2.4 MB)\n",
            "Requirement already satisfied: filelock in c:\\users\\sarfa\\documents\\pythonproject\\create-synthetic-dataset-emotion-recognition\\myenv\\lib\\site-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\users\\sarfa\\documents\\pythonproject\\create-synthetic-dataset-emotion-recognition\\myenv\\lib\\site-packages (from transformers) (0.27.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\sarfa\\documents\\pythonproject\\create-synthetic-dataset-emotion-recognition\\myenv\\lib\\site-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\sarfa\\documents\\pythonproject\\create-synthetic-dataset-emotion-recognition\\myenv\\lib\\site-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\users\\sarfa\\documents\\pythonproject\\create-synthetic-dataset-emotion-recognition\\myenv\\lib\\site-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\sarfa\\documents\\pythonproject\\create-synthetic-dataset-emotion-recognition\\myenv\\lib\\site-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in c:\\users\\sarfa\\documents\\pythonproject\\create-synthetic-dataset-emotion-recognition\\myenv\\lib\\site-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\sarfa\\documents\\pythonproject\\create-synthetic-dataset-emotion-recognition\\myenv\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\sarfa\\documents\\pythonproject\\create-synthetic-dataset-emotion-recognition\\myenv\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.12.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\sarfa\\documents\\pythonproject\\create-synthetic-dataset-emotion-recognition\\myenv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sarfa\\documents\\pythonproject\\create-synthetic-dataset-emotion-recognition\\myenv\\lib\\site-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sarfa\\documents\\pythonproject\\create-synthetic-dataset-emotion-recognition\\myenv\\lib\\site-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sarfa\\documents\\pythonproject\\create-synthetic-dataset-emotion-recognition\\myenv\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sarfa\\documents\\pythonproject\\create-synthetic-dataset-emotion-recognition\\myenv\\lib\\site-packages (from requests->transformers) (2024.12.14)\n",
            "Installing collected packages: tokenizers, transformers\n",
            "Successfully installed tokenizers-0.21.0 transformers-4.47.1\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "### install requirements ###\n",
        "%pip install --upgrade diffusers[torch]\n",
        "%pip install --upgrade transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "y4oCKFzFSwAb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading pipeline components...: 100%|██████████| 7/7 [00:06<00:00,  1.07it/s]\n"
          ]
        },
        {
          "ename": "AssertionError",
          "evalue": "Torch not compiled with CUDA enabled",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[3], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      7\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m DiffusionPipeline\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrunwayml/stable-diffusion-v1-5\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16)\n\u001b[1;32m----> 8\u001b[0m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\sarfa\\Documents\\PythonProject\\create-synthetic-dataset-emotion-recognition\\myenv\\lib\\site-packages\\diffusers\\pipelines\\pipeline_utils.py:454\u001b[0m, in \u001b[0;36mDiffusionPipeline.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    452\u001b[0m     module\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_loaded_in_4bit_bnb \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_loaded_in_8bit_bnb:\n\u001b[1;32m--> 454\u001b[0m     \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    457\u001b[0m     module\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16\n\u001b[0;32m    458\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(device) \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    459\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m silence_dtype_warnings\n\u001b[0;32m    460\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_offloaded\n\u001b[0;32m    461\u001b[0m ):\n\u001b[0;32m    462\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m    463\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipelines loaded with `dtype=torch.float16` cannot run with `cpu` device. It\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    464\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not recommended to move them to `cpu` as running them will fail. Please make\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    467\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `torch_dtype=torch.float16` argument, or use another device for inference.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    468\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\sarfa\\Documents\\PythonProject\\create-synthetic-dataset-emotion-recognition\\myenv\\lib\\site-packages\\diffusers\\models\\modeling_utils.py:1031\u001b[0m, in \u001b[0;36mModelMixin.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1026\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m is_bitsandbytes_version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.43.2\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1027\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1028\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalling `to()` is not supported for `4-bit` quantized models with the installed version of bitsandbytes. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1029\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe current device is `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`. If you intended to move the model, please install bitsandbytes >= 0.43.2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1030\u001b[0m         )\n\u001b[1;32m-> 1031\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\sarfa\\Documents\\PythonProject\\create-synthetic-dataset-emotion-recognition\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1174\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1171\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1172\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m-> 1174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\sarfa\\Documents\\PythonProject\\create-synthetic-dataset-emotion-recognition\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\sarfa\\Documents\\PythonProject\\create-synthetic-dataset-emotion-recognition\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:805\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    801\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    803\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 805\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    806\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    808\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\sarfa\\Documents\\PythonProject\\create-synthetic-dataset-emotion-recognition\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1160\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1154\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[0;32m   1155\u001b[0m             device,\n\u001b[0;32m   1156\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1157\u001b[0m             non_blocking,\n\u001b[0;32m   1158\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[0;32m   1159\u001b[0m         )\n\u001b[1;32m-> 1160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1164\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1165\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
            "File \u001b[1;32mc:\\Users\\sarfa\\Documents\\PythonProject\\create-synthetic-dataset-emotion-recognition\\myenv\\lib\\site-packages\\torch\\cuda\\__init__.py:305\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    301\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    303\u001b[0m     )\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    309\u001b[0m     )\n",
            "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
          ]
        }
      ],
      "source": [
        "### create image generation pipeline ###\n",
        "\n",
        "from diffusers import DiffusionPipeline\n",
        "import torch\n",
        "\n",
        "\n",
        "pipeline = DiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16)\n",
        "pipeline.to(\"cuda\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6MMLlWFOTes6"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'matplotlib'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[4], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      9\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/faces/happy\u001b[39m\u001b[38;5;124m'\u001b[39m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     10\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/faces/sad\u001b[39m\u001b[38;5;124m'\u001b[39m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
          ]
        }
      ],
      "source": [
        "### generate images ###\n",
        "\n",
        "import random\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "os.makedirs('/content/faces/happy', exist_ok=True)\n",
        "os.makedirs('/content/faces/sad', exist_ok=True)\n",
        "os.makedirs('/content/faces/angry', exist_ok=True)\n",
        "os.makedirs('/content/faces/surprised', exist_ok=True)\n",
        "\n",
        "\n",
        "ethnicities = ['a latino', 'a white', 'a black', 'a middle eastern', 'an indian', 'an asian']\n",
        "\n",
        "genders = ['male', 'female']\n",
        "\n",
        "emotion_prompts = {'happy': 'smiling',\n",
        "                   'sad': 'frowning, sad face expression, crying',\n",
        "                   'surprised': 'surprised, opened mouth, raised eyebrows',\n",
        "                   'angry': 'angry'}\n",
        "\n",
        "\n",
        "for j in range(250):\n",
        "\n",
        "  for emotion in emotion_prompts.keys():\n",
        "\n",
        "    emotion_prompt = emotion_prompts[emotion]\n",
        "\n",
        "    ethnicity = random.choice(ethnicities)\n",
        "    gender = random.choice(genders)\n",
        "\n",
        "    # print(emotion, ethnicity, gender)\n",
        "\n",
        "    prompt = 'Medium-shot portrait of {} {}, {}, front view, looking at the camera, color photography, '.format(ethnicity, gender, emotion_prompt) + \\\n",
        "            'photorealistic, hyperrealistic, realistic, incredibly detailed, crisp focus, digital art, depth of field, 50mm, 8k'\n",
        "    negative_prompt = '3d, cartoon, anime, sketches, (worst quality:2), (low quality:2), (normal quality:2), lowres, normal quality, ((monochrome)), ' + \\\n",
        "                      '((grayscale)) Low Quality, Worst Quality, plastic, fake, disfigured, deformed, blurry, bad anatomy, blurred, watermark, grainy, signature'\n",
        "\n",
        "    img = pipeline(prompt, negative_prompt=negative_prompt).images[0]\n",
        "\n",
        "    img.save('/content/faces/{}/{}.png'.format(emotion, str(j).zfill(4)))\n",
        "\n",
        "    # plt.imshow(img)\n",
        "    # plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_f2Ld9A3QLlP"
      },
      "outputs": [],
      "source": [
        "!zip -r faces.zip /content/faces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rh8xLPSDQR1h"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OyLNFhGQXZb"
      },
      "outputs": [],
      "source": [
        "!scp '/content/faces.zip' '/content/gdrive/My Drive/SyntheticDatasetFaceGenerationStableDiffusion/faces.zip'"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
